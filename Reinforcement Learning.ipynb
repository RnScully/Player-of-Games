{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.173534Z",
     "start_time": "2020-02-28T16:58:21.374328Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/robert/Player-of-Games/src\")\n",
    "\n",
    "\n",
    "from g2048 import Game2048\n",
    "from train_2048 import empties_state\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.207417Z",
     "start_time": "2020-02-28T16:58:23.176016Z"
    },
    "code_folding": [
     18,
     187
    ]
   },
   "outputs": [],
   "source": [
    "class RL_Player():\n",
    "    def __init__(self):\n",
    "        self.reward = 0\n",
    "        self.gamma = 0.9\n",
    "        # self.dataframe = pd.DataFrame()\n",
    "        self.short_memory = np.array([])\n",
    "        self.target = []\n",
    "        self.X = []\n",
    "        self.agent_predict = 0\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model = self.neural_net()\n",
    "        #self.model = self.network(\"weights.hdf5\")\n",
    "        self.epsilon = 0\n",
    "        self.bad_move = 0\n",
    "        self.actual = []\n",
    "        self.memory = [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]\n",
    "            \n",
    "    \n",
    "    def tokenize_board(self, board):\n",
    "        '''method takes the game board and tokenizes all the arrays into 16 other arrays for if there's a 2, a 4, an 8, etc in a location to just a 1. hopefully...makes matching better\n",
    "        Attributes \n",
    "        game( game2084 object): the game. \n",
    "        Returns\n",
    "        tokenized 256x1 tokenization of all* possible game states. \n",
    "        \n",
    "        * does not account for the 136k tile. I just..don't expect to need that. Frankly, 16k and 32k seem a reach\n",
    "        '''\n",
    "        tokenized = np.array([])\n",
    "        \n",
    "        for x in board.ravel():\n",
    "            blank = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "            if x != 0:\n",
    "                blank[int(np.log2(x))] = 1\n",
    "            tokenized = np.append(tokenized, blank)\n",
    "        return tokenized\n",
    "    \n",
    "    \n",
    "    def ai_suggest_move(self, game):\n",
    "        '''\n",
    "        method which takes the game and the model (the ai we are training) and suggests a move.\n",
    "        \n",
    "        Attributes:\n",
    "        Game(g2048 object): the game the AI is currently playing\n",
    "        \n",
    "        '''\n",
    "        self.bad_move = 0\n",
    "        board = game.board\n",
    "        \n",
    "        # tokenize_board block takes the game board and tokenizes all the arrays into 16 other arrays for if there's a 2, a 4, an 8, etc in a location to just a 1. hopefully...makes matching better\n",
    "\n",
    "        tokenized = self.tokenize_board(board) \n",
    "        \n",
    "        pred= self.model.predict(tokenized.reshape(1,-1))\n",
    "        output = [i for i in pred[0]]\n",
    "        \n",
    "        values = [2, 4, 6, 8]\n",
    "        valid_moves = game.valid_moves\n",
    "        \n",
    "        \n",
    "\n",
    "        d = dict(zip(np.array(output)[valid_moves], np.array(values)[valid_moves]))\n",
    "        \n",
    "        if len(d.keys()) == 0:\n",
    "            #print('ai_suggest_move is out of possible moves')\n",
    "            game.game_over = True #game is over\n",
    "            return  -1\n",
    "\n",
    "        if game.strict == True:\n",
    "            if max(output) in d.keys(): #if the suggested move is not in valid_moves, it wont be in d, and so this will be negative, and the game will end\n",
    "                move = d.get(max(d.keys()))\n",
    "                self.memory.pop(0)\n",
    "                output.append(move)\n",
    "                self.memory.append(np.array(output).reshape(1,-1))\n",
    "                return move\n",
    "            else:\n",
    "                game.game_over = True #game is over because this setting does not allow invaild moves:\n",
    "                return  -1\n",
    "        else:\n",
    "            if max(output) not in d.keys():\n",
    "                self.bad_move = -1 #triggers bad move if the thing has to take the next best move. \n",
    "            \n",
    "            move = d.get(max(d.keys())) #takes the next best move that is valid\n",
    "            self.memory.pop(0)\n",
    "            output.append(move)\n",
    "            self.memory.append(np.array(output).reshape(1,-1))\n",
    "            \n",
    "            return move\n",
    "    \n",
    "    def calc_reward(self, game, move):\n",
    "        '''\n",
    "        Runs right after AI suggest move\n",
    "        checks if reinforcement is merited after the last move, if so, updates the fit of model. \n",
    "        Attributes\n",
    "        game (Game2048 object)\n",
    "        move (int): int passed from ai_suggest_move()\n",
    "        \n",
    "        Updates reward\n",
    "        \n",
    "        '''\n",
    "        self.reward = 0\n",
    "\n",
    "        \n",
    "        ## get old board and new board by checking move against the game's built in moves. \n",
    "        old_board = game.board\n",
    "        \n",
    "        if move == 2: \n",
    "            next_board = game.slide_down()\n",
    "        elif move == 4:\n",
    "            next_board = game.slide_left()\n",
    "        elif move == 6:\n",
    "            next_board = game.slide_right()\n",
    "        elif move == 8:\n",
    "            next_board = game.slide_up()\n",
    "        else:\n",
    "            next_board = np.array([[ 1,  1,  1,  1],  #simple full board for end-game board state comparison. \n",
    "                                   [ 1,  1, 1, 1],\n",
    "                                    [ 1, 1, 1,  1],\n",
    "                                   [ 1,  1,  1,  1]])\n",
    "        \n",
    "        tiles_combined = empties_state(old_board, next_board)\n",
    "        \n",
    "        \n",
    "        big_tiles = dict({32:2, 64: 2, 128: 2, 256: 4, 512: 6, 1024: 8, 2048: 10, 4096: 10})\n",
    "        if np.amax(old_board) < np.amax(next_board): # checks to see if the bot has combined tiles or gotten a big one. \n",
    "            if np.amax(game.board) in big_tiles.keys():\n",
    "                self.reward += big_tiles[np.amax(game.board)]\n",
    "                print('big tiles')\n",
    "            else: \n",
    "                self.reward +=1  \n",
    "                print('biggest tile yet')\n",
    "        \n",
    "        \n",
    "        elif game.game_over == True:\n",
    "            self.reward = -10\n",
    "            print('game over penalty')\n",
    "        elif tiles_combined > 2:\n",
    "            self.reward += tiles_combined*2\n",
    "            print('board managment bonus!')\n",
    "        \n",
    "        elif self.bad_move == -1 and len(game.history) > 10: # doesn't start penalizing for bad moves untill after a few turns of play\n",
    "            self.reward = -5\n",
    "            print('invalid move')\n",
    "        self.bad_move == 0 # Reset bad move!\n",
    "        old_score = game.score\n",
    "        \n",
    "    def give_reward(self, game):\n",
    "        '''if there is a reward, this will get the game history and the memory of the outputs, multiply the outputs by the reward and its time discount\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        if self.reward != 0:\n",
    "            #print('hey, the reward is this {}, so triggering reward steps'.format(self.reward))\n",
    "            if len(game.history) < 5:\n",
    "                h = game.history\n",
    "                m = self.memory[-(len(game.history)+1):] #indexes into the last last point in memory attached to the current game.\n",
    "            else:\n",
    "                h = game.history[-6:-1]\n",
    "                m = self.memory[-6:] # memory is np array, game.history is list\n",
    "            #get moves \n",
    "            \n",
    "            \n",
    "            values = [2, 4, 6, 8]\n",
    "            discount = 1\n",
    "            self.target = []\n",
    "            for items in m:  ##make mem shotened to the length you want, currently its five. \n",
    "\n",
    "                move = items[0][-1]\n",
    "                items_short = items[0][:4]\n",
    "                k = values.index(move)\n",
    "                #value in array that corresponds to the move taken\n",
    "                \n",
    "                if self.reward*discount > 1:\n",
    "                    items_short[k] = items_short[k]*self.reward*discount\n",
    "                elif self.reward < 0:\n",
    "                    items_short[k] = items_short[k]*self.reward*discount\n",
    "                else:\n",
    "                    items_short[k] = items_short[k]*1.05\n",
    "                self.target.append(items_short)\n",
    "                discount -=.2\n",
    "                \n",
    "                \n",
    "            #$ !! tokenize the items in h\n",
    "            print('current reward is {} '.format(self.reward))\n",
    "            self.X =[self.tokenize_board(i) for i in h] #.reshape tokenize_board(i) as (1,-1) was removed\n",
    "            \n",
    "           \n",
    "            self.model.fit(np.array(self.X),np.array(self.target))\n",
    "        \n",
    "        \n",
    "    def neural_net(self, weights = None):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_dim = 256 ))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        opt = Adam(self.learning_rate)\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.355857Z",
     "start_time": "2020-02-28T16:58:23.208828Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = RL_Player()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.361573Z",
     "start_time": "2020-02-28T16:58:23.357946Z"
    }
   },
   "outputs": [],
   "source": [
    "game = Game2048(ai = True, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.373474Z",
     "start_time": "2020-02-28T16:58:23.363726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 10,468\n",
      "Trainable params: 10,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.380470Z",
     "start_time": "2020-02-28T16:58:23.376480Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_move(x):\n",
    "    if x == 6:\n",
    "        print('slide right')\n",
    "    if x == 8:\n",
    "        print('slide up')\n",
    "    if x ==4:\n",
    "        print('slide left')\n",
    "    if x == 2:\n",
    "        print('slide down')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.846251Z",
     "start_time": "2020-02-28T16:58:23.382263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new game\n",
      "\n",
      "[[0 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "[[0 0 0 0]\n",
      " [2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [0 0 0 0]]\n",
      "slide left\n",
      "\n",
      "new move\n",
      "\n",
      "[[0 0 0 0]\n",
      " [2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [0 0 0 0]]\n",
      "biggest tile yet\n",
      "current reward is 1 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 1 input samples and 2 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-375d85751d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgive_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-535436916635>\u001b[0m in \u001b[0;36mgive_reward\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2483\u001b[0m       \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m         \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    742\u001b[0m                      \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                      \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    745\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1 input samples and 2 target samples."
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(20):\n",
    "    game = Game2048(ai = True, headless = True, strict = True)\n",
    "    \n",
    "    print('new game')\n",
    "    print('')\n",
    "    while game.game_over == False:\n",
    "        \n",
    "        #print('Game Over: {}'.format(game.game_over))\n",
    "        move = agent.ai_suggest_move(game)\n",
    "        game.show_board()\n",
    "        agent.calc_reward(game, move)\n",
    "        \n",
    "        \n",
    "        agent.give_reward(game)\n",
    "        game.get_move(move)\n",
    "        game.game_step()\n",
    "        game.show_board()\n",
    "        print_move(move)\n",
    "        print('')\n",
    "        print('new move')\n",
    "        print('')\n",
    "        \n",
    "        #print('reward: {}'.format(agent.reward))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.848539Z",
     "start_time": "2020-02-28T16:58:21.410Z"
    }
   },
   "outputs": [],
   "source": [
    "game.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:34.446019Z",
     "start_time": "2020-02-28T16:58:34.426344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:47.562649Z",
     "start_time": "2020-02-28T16:58:47.550118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.24886483, 0.26566861, 0.24824138, 0.2498761 ]),\n",
       " array([0.2478528 , 0.25794205, 0.23339806, 0.27384736])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.851431Z",
     "start_time": "2020-02-28T16:58:21.447Z"
    }
   },
   "outputs": [],
   "source": [
    "game.board = np.array([[ 0,  2,  0,  0],\n",
    " [ 0,  0,  0,  0],\n",
    " [ 0,  0,  0,  4],\n",
    " [ 0,  0,  4, 16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.852756Z",
     "start_time": "2020-02-28T16:58:21.466Z"
    }
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.854353Z",
     "start_time": "2020-02-28T16:58:21.478Z"
    }
   },
   "outputs": [],
   "source": [
    "move = 6\n",
    "game.get_move(move)\n",
    "game.game_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.855387Z",
     "start_time": "2020-02-28T16:58:21.492Z"
    }
   },
   "outputs": [],
   "source": [
    "len(game.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T16:58:23.856326Z",
     "start_time": "2020-02-28T16:58:21.499Z"
    }
   },
   "outputs": [],
   "source": [
    "game.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T17:02:40.288942Z",
     "start_time": "2020-02-28T17:02:40.278382Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(game.history) < 5:\n",
    "    h = game.history\n",
    "    m = agent.memory[-(len(game.history)+1):] #indexes into the last last point in memory attached to the current game.\n",
    "else:\n",
    "    h = game.history[-6:-1]\n",
    "    m = agent.memory[-6:] # memory is np array, game.history is list\n",
    "#get moves \n",
    "\n",
    "\n",
    "values = [2, 4, 6, 8]\n",
    "discount = 1\n",
    "agent.target = []\n",
    "for items in m:  ##make mem shotened to the length you want, currently its five. \n",
    "\n",
    "    move = items[0][-1]\n",
    "    items_short = items[0][:4]\n",
    "    k = values.index(move)\n",
    "    #value in array that corresponds to the move taken\n",
    "\n",
    "    if agent.reward*discount > 1:\n",
    "        items_short[k] = items_short[k]*agent.reward*discount\n",
    "    elif agent.reward < 0:\n",
    "        items_short[k] = items_short[k]*agent.reward*discount\n",
    "    else:\n",
    "        items_short[k] = items_short[k]*1.05\n",
    "    agent.target.append(items_short)\n",
    "    discount -=.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T17:02:45.257530Z",
     "start_time": "2020-02-28T17:02:45.249938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.24886483, 0.27895204, 0.24824138, 0.2498761 ]),\n",
       " array([0.2478528 , 0.25794205, 0.23339806, 0.28753973])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T17:03:18.218226Z",
     "start_time": "2020-02-28T17:03:18.203265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv4689841a946143dd80c9fcc86c644564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
