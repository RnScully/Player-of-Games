{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:46.882620Z",
     "start_time": "2020-02-28T15:16:45.199740Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/robert/Player-of-Games/src\")\n",
    "\n",
    "\n",
    "from g2048 import Game2048\n",
    "from train_2048 import empties_state\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:46.909954Z",
     "start_time": "2020-02-28T15:16:46.884795Z"
    },
    "code_folding": [
     18,
     105,
     107,
     109
    ]
   },
   "outputs": [],
   "source": [
    "class RL_Player():\n",
    "    def __init__(self):\n",
    "        self.reward = 0\n",
    "        self.gamma = 0.9\n",
    "        # self.dataframe = pd.DataFrame()\n",
    "        self.short_memory = np.array([])\n",
    "        self.target = []\n",
    "        self.X = []\n",
    "        self.agent_predict = 0\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model = self.neural_net()\n",
    "        #self.model = self.network(\"weights.hdf5\")\n",
    "        self.epsilon = 0\n",
    "        self.bad_move = 0\n",
    "        self.actual = []\n",
    "        self.memory = [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]\n",
    "            \n",
    "    \n",
    "    def tokenize_board(self, board):\n",
    "        '''method takes the game board and tokenizes all the arrays into 16 other arrays for if there's a 2, a 4, an 8, etc in a location to just a 1. hopefully...makes matching better\n",
    "        Attributes \n",
    "        game( game2084 object): the game. \n",
    "        Returns\n",
    "        tokenized 256x1 tokenization of all* possible game states. \n",
    "        \n",
    "        * does not account for the 136k tile. I just..don't expect to need that. Frankly, 16k and 32k seem a reach\n",
    "        '''\n",
    "        tokenized = np.array([])\n",
    "        \n",
    "        for x in board.ravel():\n",
    "            blank = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "            if x != 0:\n",
    "                blank[int(np.log2(x))] = 1\n",
    "            tokenized = np.append(tokenized, blank)\n",
    "        return tokenized\n",
    "    \n",
    "    \n",
    "    def ai_suggest_move(self, game):\n",
    "        '''\n",
    "        method which takes the game and the model (the ai we are training) and suggests a move.\n",
    "        \n",
    "        Attributes:\n",
    "        Game(g2048 object): the game the AI is currently playing\n",
    "        \n",
    "        '''\n",
    "        self.bad_move = 0\n",
    "        board = game.board\n",
    "        \n",
    "        # tokenize_board block takes the game board and tokenizes all the arrays into 16 other arrays for if there's a 2, a 4, an 8, etc in a location to just a 1. hopefully...makes matching better\n",
    "\n",
    "        tokenized = self.tokenize_board(board) \n",
    "        \n",
    "        pred= self.model.predict(tokenized.reshape(1,-1))\n",
    "        output = [i for i in pred[0]]\n",
    "        \n",
    "        values = [2, 4, 6, 8]\n",
    "        valid_moves = game.valid_moves\n",
    "        \n",
    "        \n",
    "\n",
    "        d = dict(zip(np.array(output)[valid_moves], np.array(values)[valid_moves]))\n",
    "        \n",
    "        if len(d.keys()) == 0:\n",
    "            #print('ai_suggest_move is out of possible moves')\n",
    "            game.game_over = True #game is over\n",
    "            return  -1\n",
    "\n",
    "        if game.strict == True:\n",
    "            if max(output) in d.keys(): #if the suggested move is not in valid_moves, it wont be in d, and so this will be negative, and the game will end\n",
    "                move = d.get(max(d.keys()))\n",
    "                self.memory.pop(0)\n",
    "                output.append(move)\n",
    "                self.memory.append(np.array(output).reshape(1,-1))\n",
    "                return move\n",
    "            else:\n",
    "                game.game_over = True #game is over because this setting does not allow invaild moves:\n",
    "                return  -1\n",
    "        else:\n",
    "            self.bad_move = -1\n",
    "            move = d.get(max(d.keys())) #takes the next best move that is valid\n",
    "            self.memory.pop(0)\n",
    "            output.append(move)\n",
    "            self.memory.append(np.array(output).reshape(1,-1))\n",
    "            \n",
    "            return move\n",
    "    \n",
    "    def calc_reward(self, game, move):\n",
    "        '''\n",
    "        Runs right after AI suggest move\n",
    "        checks if reinforcement is merited after the last move, if so, updates the fit of model. \n",
    "        Attributes\n",
    "        game (Game2048 object)\n",
    "        move (int): int passed from ai_suggest_move()\n",
    "        \n",
    "        Updates reward\n",
    "        \n",
    "        '''\n",
    "        self.reward = 0\n",
    "\n",
    "        \n",
    "        ## get old board and new board by checking move against the game's built in moves. \n",
    "        old_board = game.board\n",
    "        \n",
    "        if move == 2: \n",
    "            next_board = game.slide_down()\n",
    "        elif move == 4:\n",
    "            next_board = game.slide_left()\n",
    "        elif move == 6:\n",
    "            next_board = game.slide_right()\n",
    "        elif move == 8:\n",
    "            next_board = game.slide_up()\n",
    "        \n",
    "        tiles_combined = empties_state(old_board, next_board)\n",
    "        \n",
    "        \n",
    "        big_tiles = dict({32:2, 64: 2, 128: 2, 256: 4, 512: 6, 1024: 8, 2048: 10, 4096: 10})\n",
    "        if np.amax(old_board) < np.amax(next_board): # checks to see if the bot has combined tiles or gotten a big one. \n",
    "            if np.amax(game.board) in big_tiles.keys():\n",
    "                self.reward += big_tiles[np.amax(game.board)]\n",
    "            else: \n",
    "                self.reward +=1  \n",
    "        \n",
    "        \n",
    "        if game.game_over == True:\n",
    "            self.reward = -10\n",
    "        elif tiles_combined > 2:\n",
    "            self.reward += 2\n",
    "        \n",
    "        elif self.bad_move == -1: # the game is over or an illegal move was made\n",
    "            self.reward = -2\n",
    "        old_score = game.score\n",
    "        \n",
    "    def give_reward(self, game):\n",
    "        '''if there is a reward, this will get the game history and the memory of the outputs, multiply the outputs by the reward and its time discount\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        if self.reward != 0:\n",
    "            #print('hey, the reward is this {}, so triggering reward steps'.format(self.reward))\n",
    "            if len(game.history) < 5:\n",
    "                h = game.history\n",
    "                m = self.memory[-(len(game.history)+1):-1] #indexes into the last last point in memory attached to the current game.\n",
    "            else:\n",
    "                h = game.history[-6:-1]\n",
    "                m = self.memory[-6:] # memory is np array, game.history is list\n",
    "            #get moves \n",
    "            \n",
    "            \n",
    "            values = [2, 4, 6, 8]\n",
    "            discount = 1\n",
    "            self.target = []\n",
    "            for items in m:  ##make mem shotened to the length you want, currently its five. \n",
    "\n",
    "                move = items[0][-1]\n",
    "                items_short = items[0][:4]\n",
    "                k = values.index(move)\n",
    "                #value in array that corresponds to the move taken\n",
    "                \n",
    "                if self.reward*discount > 1:\n",
    "                    items_short[k] = items_short[k]*self.reward*discount\n",
    "                else:\n",
    "                    items_short[k] = items_short[k]*1.05\n",
    "                self.target.append(items_short)\n",
    "                discount -=.2\n",
    "                \n",
    "                \n",
    "            #$ !! tokenize the items in h\n",
    "            print('current reward is {} '.format(self.reward))\n",
    "            self.X =[self.tokenize_board(i) for i in h] #.reshape tokenize_board(i) as (1,-1) was removed\n",
    "\n",
    "            agent.model.fit(np.array(self.X),np.array(self.target))\n",
    "        \n",
    "        \n",
    "    def neural_net(self, weights = None):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_dim = 256 ))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        opt = Adam(self.learning_rate)\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.025692Z",
     "start_time": "2020-02-28T15:16:46.911787Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = RL_Player()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.030436Z",
     "start_time": "2020-02-28T15:16:47.027323Z"
    }
   },
   "outputs": [],
   "source": [
    "game = Game2048(ai = True, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.041326Z",
     "start_time": "2020-02-28T15:16:47.032338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 10,468\n",
      "Trainable params: 10,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:20:47.431702Z",
     "start_time": "2020-02-28T15:20:47.367815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "new game\n",
      "\n",
      "[[0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "[[0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "move is: 4\n",
      "current reward is -2 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (256,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c6699b1c93e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'move is: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgive_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1afc00038648>\u001b[0m in \u001b[0;36mgive_reward\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#.reshape tokenize_board(i) as (1,-1) was removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (256,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(2000):\n",
    "    game = Game2048(ai = True, headless = True, strict = False)\n",
    "    game.show_board()\n",
    "    print('new game')\n",
    "    print('')\n",
    "    while game.game_over == False:\n",
    "        #print('Game Over: {}'.format(game.game_over))\n",
    "        move = agent.ai_suggest_move(game)\n",
    "        game.show_board()\n",
    "        agent.calc_reward(game, move)\n",
    "        game.show_board()\n",
    "        print('move is: {}'.format(move))\n",
    "        agent.give_reward(game)\n",
    "        game.get_move(move)\n",
    "        game.game_step()\n",
    "\n",
    "        #print('reward: {}'.format(agent.reward))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:17:01.222379Z",
     "start_time": "2020-02-28T15:17:01.189585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.450166Z",
     "start_time": "2020-02-28T15:16:45.223Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.451310Z",
     "start_time": "2020-02-28T15:16:45.228Z"
    }
   },
   "outputs": [],
   "source": [
    "h = game.history[-6:-1]\n",
    "m = agent.memory[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.452842Z",
     "start_time": "2020-02-28T15:16:45.234Z"
    }
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.454012Z",
     "start_time": "2020-02-28T15:16:45.239Z"
    }
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T15:16:47.455064Z",
     "start_time": "2020-02-28T15:16:45.244Z"
    }
   },
   "outputs": [],
   "source": [
    "len(game.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv4689841a946143dd80c9fcc86c644564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
