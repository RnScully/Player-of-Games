{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:30:19.218611Z",
     "start_time": "2020-02-27T22:30:17.818993Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/robert/Player-of-Games/src\")\n",
    "\n",
    "\n",
    "from g2048 import Game2048\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:30:19.235387Z",
     "start_time": "2020-02-27T22:30:19.220098Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RL_Player():\n",
    "    def __init__(self):\n",
    "            self.reward = 0\n",
    "            self.gamma = 0.9\n",
    "            # self.dataframe = pd.DataFrame()\n",
    "            self.short_memory = np.array([])\n",
    "            self.agent_target = 1\n",
    "            self.agent_predict = 0\n",
    "            self.learning_rate = 0.0005\n",
    "            self.model = self.neural_net()\n",
    "            #self.model = self.network(\"weights.hdf5\")\n",
    "            self.epsilon = 0\n",
    "            self.actual = []\n",
    "            self.memory = []\n",
    "            \n",
    "    \n",
    "    def tokenize_board(self, board):\n",
    "        '''method takes the game board and tokenizes all the arrays into 16 other arrays for if there's a 2, a 4, an 8, etc in a location to just a 1. hopefully...makes matching better\n",
    "        Attributes \n",
    "        board (np.array): 4x4 Game2048 board object\n",
    "        Returns\n",
    "        tokenized 256x1 tokenization of all* possible game states. \n",
    "        \n",
    "        * does not account for the 136k tile. I just..don't expect to need that. Frankly, 16k and 32k seem a reach\n",
    "        '''\n",
    "        tokenized = np.array([])\n",
    "        \n",
    "        for x in game.board.ravel():\n",
    "            blank = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "            if x != 0:\n",
    "                blank[int(np.log2(x))] = 1\n",
    "        tokenized = np.append(tokenized, blank)\n",
    "        return tokenized\n",
    "    \n",
    "    \n",
    "    def ai_suggest_move(self, game):\n",
    "        '''\n",
    "        method which takes the game and the model (the ai we are training) and suggests a move.\n",
    "        \n",
    "        Attributes:\n",
    "        Game(g2048 object): the game the AI is currently playing\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        board = game.board.ravel()\n",
    "        \n",
    "        # tokenize_board block takes the game board and tokenizes all the arrays into 16 other arrays for if there's a 2, a 4, an 8, etc in a location to just a 1. hopefully...makes matching better\n",
    "\n",
    "        tokenized = self.tokenize_board(board) \n",
    "        \n",
    "        pred= self.model.predict(tokenized.reshape(1,-1))\n",
    "        output = [i for i in pred[0]]\n",
    "        \n",
    "        values = [2, 4, 6, 8]\n",
    "        valid_moves = game.valid_moves\n",
    "        \n",
    "        \n",
    "\n",
    "        d = dict(zip(np.array(output)[valid_moves], np.array(values)[valid_moves]))\n",
    "        if len(d.keys()) == 0:\n",
    "            #print('ai_suggest_move is out of possible moves')\n",
    "            game.game_over = True #game is over\n",
    "            return  -1\n",
    "\n",
    "        if game.strict == True:\n",
    "            if max(output) in d.keys(): #if the suggested move is not in valid_moves, it wont be in d, and so this will be negative, and the game will end\n",
    "                move = d.get(max(d.keys()))\n",
    "                self.memory.pop(0)\n",
    "                self.memory.append(output)\n",
    "                return move\n",
    "            else:\n",
    "                game.game_over = True #game is over because this setting does not allow invaild moves:\n",
    "                return  -1\n",
    "        else:\n",
    "            move = d.get(max(d.keys())) #takes the next best move that is valid\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append(output) #$$! we could get clever and reward the correct move more. \n",
    "            return move\n",
    "    \n",
    "    def give_reward(self, game):\n",
    "        \n",
    "        game.history()\n",
    "        \n",
    "    def neural_net(self, weights = None):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_shape= (256,)  ))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        opt = Adam(self.learning_rate)\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:30:19.366692Z",
     "start_time": "2020-02-27T22:30:19.237025Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = RL_Player()\n",
    "game = Game2048(ai = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:30:19.374759Z",
     "start_time": "2020-02-27T22:30:19.368568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 10,468\n",
      "Trainable params: 10,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:30:19.379333Z",
     "start_time": "2020-02-27T22:30:19.376733Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:30:19.455485Z",
     "start_time": "2020-02-27T22:30:19.380905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:31:31.789954Z",
     "start_time": "2020-02-27T22:31:31.777719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [0 4 0 2]\n",
      " [0 0 2 8]\n",
      " [0 0 2 4]]\n"
     ]
    }
   ],
   "source": [
    "game.get_move()\n",
    "game.game_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:38:24.254053Z",
     "start_time": "2020-02-27T22:38:24.235756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 2, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 2, 0, 4]),\n",
       " array([0, 0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 0, 2, 4, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 8, 0, 0, 2, 4]),\n",
       " array([0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 2, 8, 0, 0, 2, 4])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = game.history\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:32:54.989134Z",
     "start_time": "2020-02-27T22:32:54.983511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 2, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 2, 0, 4]),\n",
       " array([0, 0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 0, 2, 4, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 8, 0, 0, 2, 4])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_h = game.hisory[-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:44:12.978662Z",
     "start_time": "2020-02-27T22:44:12.961119Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def update_lmem(x):\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:43:21.947736Z",
     "start_time": "2020-02-27T22:43:21.936537Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:45:15.020987Z",
     "start_time": "2020-02-27T22:45:15.006522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 3], [11, 3], [11, 3], [11, 3]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T22:45:13.239231Z",
     "start_time": "2020-02-27T22:45:13.224592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 3], [11, 3], [11, 3], [11, 3]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def give_reward(self, game):\n",
    "        '''\n",
    "        checks if reinforcement is merited after the last move, if so, updates the fit of model. \n",
    "        '''\n",
    "        game.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv4689841a946143dd80c9fcc86c644564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
